{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.sparse import csr_matrix\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "QueryFolderPath = 'ntust-ir-2020_hw4_v2/queries'\n",
    "QueryNames = [pathimfor[2] for pathimfor in os.walk(QueryFolderPath)][0]\n",
    "QueryPaths = [os.path.join(QueryFolderPath,QueryName) for QueryName in QueryNames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocumentFolderPath = 'ntust-ir-2020_hw4_v2/docs'\n",
    "DocumentNames = [pathimfor[2] for pathimfor in os.walk(DocumentFolderPath)][0]\n",
    "DocumentPaths = [os.path.join(DocumentFolderPath,DocumentName) for DocumentName in DocumentNames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetValcabularyAndCalculateTF():\n",
    "    Corpus_dict = {}\n",
    "    Corpus_index = 0\n",
    "    DocumentTF_list = []\n",
    "    BackGround = {}\n",
    "    \n",
    "    T = 0\n",
    "    \n",
    "    for index,DocumentPath in enumerate(DocumentPaths):\n",
    "        TF = {}\n",
    "        with open(DocumentPath,'r') as file:\n",
    "            Terms = file.read().split()\n",
    "        \n",
    "        for Term in Terms:\n",
    "            TF[Term] = 1 if Term not in TF else TF[Term] + 1\n",
    "            if Term not in Corpus_dict:\n",
    "                Corpus_dict[Term] = Corpus_index\n",
    "                Corpus_index += 1\n",
    "            if Term not in BackGround:\n",
    "                BackGround[Term] = 1\n",
    "            else:\n",
    "                BackGround[Term] += 1\n",
    "        DocumentTF_list.append(TF)\n",
    "    \n",
    "        \n",
    "        print(f\"finish document:{index}\",end = '\\r')\n",
    "        T += 1\n",
    "        if T == 5000:\n",
    "            break\n",
    "    return Corpus_dict,DocumentTF_list,BackGround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conver TF Dictionary to List\n",
    "def ConvertTFDict_to_List(Corpus_dict,DocumentTF_list):\n",
    "    length = len(Corpus_dict)\n",
    "    for index,DocumentTF in enumerate(DocumentTF_list):\n",
    "        tempTF = [0]*length\n",
    "        for Term in DocumentTF:\n",
    "            tempTF[Corpus_dict[Term]] = DocumentTF[Term]\n",
    "        DocumentTF_list[index] = tempTF\n",
    "            \n",
    "    return DocumentTF_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConverBGTerm_to_ID(Corpus,BackGround):\n",
    "    length = len(Corpus)\n",
    "    tempBG = {}\n",
    "    for Term in BackGround:\n",
    "        tempBG[Corpus[Term]] = BackGround[Term]\n",
    "    for Term in tempBG:\n",
    "        tempBG[Term] /= sum(tempBG.values())\n",
    "    return tempBG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLSA:\n",
    "    def __init__(self,TopicNum,DocumentNum,WordNum,DocumentTF_list,Corpus,BackGround):\n",
    "        self.TopicNum = TopicNum\n",
    "        self.DocumentNum = DocumentNum\n",
    "        self.WordNum = WordNum\n",
    "        self.DocumentTF_list = DocumentTF_list\n",
    "        self.Corpus = Corpus\n",
    "        self.BackGround = BackGround\n",
    "        self.P_TkWiDj = []\n",
    "        self.P_WiTk = []\n",
    "        self.P_TkDj = []\n",
    "        \n",
    "    @numba.jit()\n",
    "    def InitialP_TkWiDj(self):\n",
    "        for j in range(self.DocumentNum):\n",
    "            temp_dict = {}\n",
    "            for i in range(self.WordNum):\n",
    "                temp_dict[i] = [1/TopicNum] * TopicNum\n",
    "            self.P_TkWiDj.append(temp_dict)\n",
    "        \n",
    "    @numba.jit()\n",
    "    def InitialP_WiTk(self):\n",
    "        for k in range(self.TopicNum):\n",
    "            self.P_WiTk.append([1/self.WordNum]*WordNum)\n",
    "                \n",
    "    @numba.jit()\n",
    "    def InitialP_TkDj(self):\n",
    "        for j in range(self.DocumentNum):\n",
    "            self.P_TkDj.append([1/self.TopicNum] * TopicNum)\n",
    "                \n",
    "    @numba.jit()\n",
    "    def Initial(self):\n",
    "        self.InitialP_TkWiDj()\n",
    "        self.InitialP_WiTk()\n",
    "        self.InitialP_TkDj()\n",
    "\n",
    "    @numba.jit()\n",
    "    def E_step(self):\n",
    "        for j in range(self.DocumentNum):\n",
    "            for i in range(self.WordNum):\n",
    "                Denominator = 0\n",
    "                for k in range(self.TopicNum):\n",
    "                    self.P_TkWiDj[j][i][k] = self.P_WiTk[k][i] * self.P_TkDj[j][k]\n",
    "                    Denominator += self.P_TkWiDj[j][i][k]\n",
    "                for k in range(self.TopicNum):\n",
    "                    self.P_TkWiDj[j][i][k] /= Denominator\n",
    "                    \n",
    "    @numba.jit()\n",
    "    def M_step(self):\n",
    "        for k in range(self.TopicNum):\n",
    "            Denominator = 0\n",
    "            for i in range(self.WordNum):\n",
    "                for j in range(self.DocumentNum):\n",
    "                    if self.DocumentTF_list[j][i] !=0:\n",
    "                        self.P_WiTk[k][i] += self.DocumentTF_list[j][i] * self.P_TkWiDj[j][i][k]\n",
    "                Denominator += self.P_WiTk[k][i]\n",
    "            for i in range(self.WordNum):\n",
    "                self.P_WiTk[k][i] /= Denominator\n",
    "\n",
    "        for j in range(self.DocumentNum):\n",
    "            for k in range(self.TopicNum):\n",
    "                for i in range(self.WordNum):\n",
    "                    if self.DocumentTF_list[j][i] != 0:\n",
    "                        self.P_TkDj[j][k] += self.DocumentTF_list[j][i] * self.P_TkWiDj[j][i][k]\n",
    "                self.P_TkDj[j][k] /= sum(self.DocumentTF_list[j])\n",
    "                \n",
    "    def ComputeSimilarity(self,QueryTF,alpha,beta):\n",
    "        Score = {}\n",
    "        for j in range(self.DocumentNum):\n",
    "            similarity = 1\n",
    "            DocumentName = DocumentNames[j][:-4]\n",
    "            for QueryIndex,(Queryi,QueryTF_Num) in enumerate(QueryTF.items()):\n",
    "                if(Queryi != 'None'):\n",
    "                    Document_value = self.DocumentTF_list[j][Queryi] / sum(self.DocumentTF_list[j])\n",
    "\n",
    "                    PLSA_value = 0\n",
    "                    for k in range(self.TopicNum):\n",
    "                        PLSA_value += self.P_WiTk[k][Queryi] * self.P_TkDj[j][k]\n",
    "\n",
    "                    BackGround_value =  self.BackGround[Queryi]\n",
    "\n",
    "\n",
    "                    similarity *= alpha * Document_value + beta * PLSA_value + (1-alpha-beta)*BackGround_value\n",
    "            Score[DocumentName] = similarity\n",
    "        Score = sorted(Score.items(), key=lambda x:x[1])\n",
    "        Score.reverse()\n",
    "        return Score[:1000]\n",
    "            \n",
    "    @numba.jit()\n",
    "    def CalculateLoss(self):\n",
    "        loss = 0\n",
    "        for j in range(self.DocumentNum):\n",
    "            for i in range(self.WordNum):\n",
    "                temp_loss = 0\n",
    "                if self.DocumentTF_list[j][i] > 0:\n",
    "                    for k in range(self.TopicNum):\n",
    "                        temp_loss += self.P_WiTk[k][i] * self.P_TkDj[j][k]\n",
    "                    if temp_loss > 0:\n",
    "#                         temp_loss /= self.DocumentNum\n",
    "                        loss += np.log(temp_loss) * self.DocumentTF_list[j][i]\n",
    "        return -loss\n",
    "                \n",
    "    \n",
    "    def EM_Algorithm(self,epochs):\n",
    "        for epoch in range(epochs):\n",
    "            self.E_step()\n",
    "            finishtime = time.strftime('%X')\n",
    "            self.M_step()\n",
    "            finishtime = time.strftime('%X')\n",
    "            loss = self.CalculateLoss()\n",
    "            finishtime = time.strftime('%X')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GetValcabulary\n",
    "Corpus,DocumentTF_list,BackGround = GetValcabularyAndCalculateTF()\n",
    "# set parameter\n",
    "TopicNum = 4\n",
    "DocumentNum = len(DocumentNames)\n",
    "WordNum = len(Corpus)\n",
    "epochs = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocumentNum = len(DocumentNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocumentTF_list = ConvertTFDict_to_List(Corpus,DocumentTF_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BackGround = ConverBGTerm_to_ID(Corpus,BackGround)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TopicNum = 2\n",
    "epochs = 6\n",
    "Model = PLSA(TopicNum,DocumentNum,WordNum,DocumentTF_list,Corpus,BackGround)\n",
    "# del DocumentTF_list\n",
    "# del BackGround"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Model.Initial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.EM_Algorithm(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeQueryTF(path):\n",
    "    with open(path,'r') as file:\n",
    "        Terms = file.read().split()\n",
    "    TF = {}\n",
    "    for term in Terms:\n",
    "        if term not in TF:\n",
    "            TF[term] = 1\n",
    "        else:\n",
    "            TF[term] += 1\n",
    "    return TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertQueryTermDict_to_ID(Corpus_dict,QueryTF):\n",
    "    tempTF = {}\n",
    "    for term in QueryTF:\n",
    "        if term in Corpus:\n",
    "            tempTF[Corpus_dict[term]] = QueryTF[term]\n",
    "        else:\n",
    "            tempTF['None'] = 0\n",
    "    return tempTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "File = open('001.txt','w')\n",
    "File.write('Query,RetrievedDocuments')\n",
    "for index,path in enumerate(QueryPaths):\n",
    "    print(f\"start:{index}\")\n",
    "    QueryFileName = QueryNames[index][:-4]\n",
    "    File.writelines('\\n' + QueryFileName + ',')\n",
    "    \n",
    "    QueryTF = ComputeQueryTF(path)\n",
    "    QueryTF = ConvertQueryTermDict_to_ID(Corpus,QueryTF)\n",
    "    print(f\"start compute\")\n",
    "    Score = Model.ComputeSimilarity(QueryTF,.8,.1)\n",
    "    for (DocumentName,score) in Score:\n",
    "        File.write(DocumentName + ' ')\n",
    "File.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
